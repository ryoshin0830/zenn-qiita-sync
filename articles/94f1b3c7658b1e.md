---
title: 巨大言語モデルQwen3-235Bをフルスペックで動かすためのGPU選定ガイド
emoji: "\U0001F680"
type: tech
topics:
  - LLM
  - GPU
  - Qwen
  - AI
  - インフラ
published: true
---
# はじめに

2025年5月に発表されたQwen3-235B-A22Bは、2350億パラメータを持つ最新の大規模言語モデルです。その性能は魅力的ですが、フルスペックで動かすには相当なハードウェアが必要です。

本記事では、Vast.aiを使ってQwen3-235Bを**量子化なし**で動かすための具体的な構成と費用を調査した結果をまとめました。

# Qwen3-235Bの特徴

Qwen3-235B-A22Bの主な特徴：
- **パラメータ数**: 2350億（総数）、220億（アクティブ）
- **コンテキスト長**: 32,768トークン（ネイティブ）、131,072トークン（YaRN使用時）
- **エキスパート数**: 128（アクティブ: 8）
- **思考モード**: 複雑な推論タスクに対応する思考モードを搭載

# 必要なハードウェア要件

## メモリ要件の計算

```
基本計算：
- パラメータ数: 235B
- BF16（2バイト/パラメータ）: 235B × 2 = 470GB

推論時の追加メモリ：
- KVキャッシュ: 約50-100GB
- 中間活性化: 約50GB
- その他のバッファ: 約50GB

合計必要メモリ: 620-670GB
```

## GPU構成の最小要件

- **最小構成**: 8 × H100 80GB

# Vast.aiでの価格調査結果

## 利用可能な構成と価格

### H100構成（フルスペック動作可能）

| Type | 構成 | 価格/時間 | メモリ総量 | 信頼性 |
|------|------|-----------|------------|--------|
| #19476706 (Texas) | 8×H100 SXM | **$13.87** | 640GB | 99.52% |
| #10387195 (Netherlands) | 8×H100 SXM | $16.00 | 640GB | 99.96% |
| #19488926 (US) | 8×H100 SXM | $17.07 | 640GB | 99.89% |
| #19663404 (US) | 8×H100 SXM | $18.14 | 640GB | 99.87% |

### H200構成（余裕のあるメモリ）

| Type | 構成 | 価格/時間 | メモリ総量 | 信頼性 |
|------|------|-----------|------------|--------|
| #20417150 (US) | 8×H200 | $22.59 | 1,120GB | 99.90% |
| #22370397 (Iceland) | 8×H200 | $28.24 | 1,120GB | 97.8% |

## コスト計算例

### 短期テスト
```
H100構成（最安）:
- 1時間: $13.87
- 4時間: $55.48
- 8時間: $110.96
- 24時間: $332.88
```

### 開発期間での利用
```
1週間（40時間稼働）: $554.80
1週間（24/7稼働）: $2,330.16
1ヶ月（24/7稼働）: 約$10,000
```

# 他のGPUでは動かせない理由

## A100での実現可能性

- **A100 80GB × 8台 = 640GB**: ギリギリで不安定
- **A100 40GB × 8台 = 320GB**: 完全に不足（メモリが半分以下）

Vast.aiで利用可能なA100の多くは40GB版のため、実質的に不可能です。

## その他のGPU

| GPUタイプ | メモリ/枚 | 8枚合計 | 判定 |
|----------|-----------|---------|------|
| L40S | 45GB | 360GB | ❌ 不足 |
| A40 | 45GB | 360GB | ❌ 不足 |
| A10 | 22GB | 176GB | ❌ 完全に不足 |
| Tesla V100 | 32GB | 256GB | ❌ 完全に不足 |

# 商用APIとの比較

## なぜ商用APIは安いのか？

### 1. 規模の経済
- 数万〜数十万のGPUを保有
- GPU稼働率: 90%以上（個人利用なら30-50%）
- 1つのGPUで数百人を同時処理

### 2. 技術的最適化
- INT8/INT4量子化で効率化
- キャッシング（頻出質問の20-30%は再計算不要）
- 動的ルーティング（簡単な質問は小型モデルへ）

### 3. ビジネスモデル
- 無料/安価なティア: 赤字
- エンタープライズ: 高収益
- 投資資金による補填

## 複数人利用時の課題

個人でGPUをレンタルした場合、複数人で使うと性能が線形に低下します：

```
ユーザー数による速度低下（バッチ処理なし）:
- 1人: 150 tokens/秒
- 2人: 各75 tokens/秒
- 4人: 各37.5 tokens/秒
- 10人: 各15 tokens/秒（ChatGPTより遅い）
```

適切なバッチ処理の実装には、vLLMなどの専門的なツールと技術が必要です。

# Interruptibleインスタンスの利用

## 大幅なコスト削減が可能

Interruptible（中断可能）インスタンスを使うと、最大73%の割引が受けられます：

```
例: 4×H100（Thailand）
- On-Demand: $6.94/時間
- Interruptible: $1.88/時間（73%割引）
```

ただし、以下の制約があります：
- ホストが必要な時に即座に停止される
- 通常30秒〜5分前の通知
- データ損失のリスク

短時間のテストや中断しても問題ないタスクには最適です。

# AMD GPUは使えるか？

技術的には可能ですが、実用的ではありません：

## 問題点
1. **ソフトウェアエコシステム**: PyTorch、Transformersの対応が限定的
2. **パフォーマンス**: 最適化不足で2-3倍遅い
3. **サポート**: コミュニティが極小、デバッグ情報が少ない
4. **Vast.aiでの提供**: AMD GPUのオファーはほぼゼロ

現時点でLLM運用はNVIDIA一択です。

# まとめと推奨構成

## フルスペック運用の推奨

1. **コスト重視**: Type #19476706（8×H100）- **$13.87/時間**
2. **安定性重視**: Type #20417150（8×H200）- **$22.59/時間**
3. **バランス型**: Type #10387195（8×H100）- **$16.00/時間**

## 代替案

フルスペックが予算的に厳しい場合：

1. **より小さいモデル**
   - Qwen2.5-72B: L40S×4枚で動作可能（$3.20/時間）
   - Qwen2.5-32B: L40S×2枚で動作可能（$1.60/時間）

2. **量子化版の使用**
   - INT8量子化でメモリ使用量を半減
   - 性能は低下するが、より安価なGPUで動作可能

3. **商用APIの利用**
   - 少人数チームなら圧倒的にコスト効率的
   - 月額$20程度で利用可能

## 最終的な判断基準

- **個人研究/実験**: 自前GPU（短期利用）✓
- **5人以下のチーム**: 商用API推奨
- **特殊要件あり**: 自前GPU検討
- **一般業務利用**: 商用API一択

Qwen3-235Bは素晴らしいモデルですが、フルスペックでの運用にはかなりのコストがかかります。用途と予算に応じて、適切な選択をすることが重要です。
